#!./.IOT/bin/python
import os
from math import floor
import fire
import torch
import pandas as pd

TRAIN_DATA_FRAC = 0.8


def process_to_tensors(train_data, test_data):
    x_train = train_data.drop("label", axis=1)
    y_train = train_data["label"]
    x_test = test_data.drop("label", axis=1)
    y_test = test_data["label"]

    # drop the index column but it's unnamed
    x_train = x_train.drop("Unnamed: 0", axis=1)
    x_test = x_test.drop("Unnamed: 0", axis=1)

    x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

    return (x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor)


def splitset(dataset, parts):
    n = dataset.shape[0]
    local_n = floor(n / parts)
    # calculate leftover data points
    leftover = n % parts

    lengths = [local_n + 1 if i < leftover else local_n for i in range(parts)]

    # result = torch.utils.data.random_split(dataset, lengths. torch.Generator().manual_seed(42))
    result = []

    start = 0
    for length in lengths:
        result.append(dataset[start : start + length])
        start += length

    return result


# vertically split the data into n_splits
def splitset_v(dataset, parts, label=False):
    print(f"dataset shape: {dataset.shape}")
    n = dataset.shape[1]
    local_n = floor(n / parts)
    # calculate leftover data points
    leftover = n % parts

    lengths = [local_n + 1 if i < leftover else local_n for i in range(parts)]

    # result = torch.utils.data.random_split(dataset, lengths. torch.Generator().manual_seed(42))
    result = []

    start = 0
    for length in lengths:
        result.append(dataset[:, start : start + length])
        start += length

    return result


def split(out_dir="bin/data", n_splits=5):
    # Make dir
    if not os.path.exists(f"{out_dir}/clients"):
        os.mkdir(f"{out_dir}/clients")

    # Load and convert to dict
    normal_data = pd.read_csv(f"{out_dir}/nodes5_normal.csv")
    base_data = pd.read_csv(f"{out_dir}/nodes5_base.csv")

    # Concat both data
    all_data = pd.concat([normal_data, base_data], ignore_index=True)

    #### train and test data split

    # extract data points with label 1 and 0 from normal and base data respectively
    all_data_1 = all_data[all_data["label"] == 1]
    all_data_0 = all_data[all_data["label"] == 0]

    # generate train and test data with 80 to 20 ratio from each lable
    train_data_1 = all_data_1.sample(frac=TRAIN_DATA_FRAC)
    test_data_1 = all_data_1.drop(train_data_1.index)
    train_data_0 = all_data_0.sample(frac=TRAIN_DATA_FRAC)
    test_data_0 = all_data_0.drop(train_data_0.index)

    # concatenate train and test data
    train_data_df = pd.concat([train_data_1, train_data_0], ignore_index=True)
    test_data_df = pd.concat([test_data_1, test_data_0], ignore_index=True)

    # initialize train_data and test_data tensors
    train_data = torch.utils.data.TensorDataset()
    test_data = torch.utils.data.TensorDataset()

    train_data.data, train_data.targets, test_data.data, test_data.targets = (
        process_to_tensors(train_data_df, test_data_df)
    )

    # shuffle the train data

    # split each of train and test data into n_splits
    # data = {
    #     "x_train": splitset(train_data.data, n_splits),
    #     "y_train": splitset(train_data.targets, n_splits),
    #     "x_test": splitset(test_data.data, n_splits),
    #     "y_test": splitset(test_data.targets, n_splits),
    # }

    # split each of train and test data into n_splits vertically
    data = {
        "x_train": splitset_v(train_data.data, n_splits),
        "y_train": train_data.targets,
        "x_test": splitset_v(test_data.data, n_splits),
        "y_test": test_data.targets,
    }

    print(f'x_train[0]: {data["x_train"][0].shape}')
    print(f'y_train[0]: {data["y_train"].shape}')
    print(f'x_test[4]: {data["x_test"][4].shape}')
    print(f'y_test[4]: {data["y_test"].shape}')

    # Make splits
    for i in range(n_splits):
        subdir = f"{out_dir}/clients/{str(i+1)}"
        if not os.path.exists(subdir):
            os.mkdir(subdir)
        torch.save(
            {
                "x_train": data["x_train"][i],
                "x_test": data["x_test"][i],
            },
            f"{subdir}/IOT_normal_base.pt",
        )
    torch.save(
        {
            "y_train": data["y_train"][0],
            "y_test": data["y_test"][0],
        },
        f"../../fedn/combiner_data/IOT_normal_base_labels.pt",
    )


if __name__ == "__main__":
    fire.Fire(split)
